{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilyadamek8/graph1/blob/master/Assignment%20AI%20Week%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y4vNTFdfGAOn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "The dataset that I have chosen is focused on a Zoo and all of the animals inside of it. It contains the answers to a list of 18 questions for each animal of 101 animals inside the zoo, of which there is animal name, 15 Boolean attributes and also 2 numeric integer attributes (both have a range in which they can be answered).  The dataset therefore displays and records all necessary attributes about each animal in a zoo and allows the user to try and identify an animal based on its attributes. \n",
        "NB: For the purpose of this dataset, 0 represents false for the Boolean attributes and 1 represents true.\n",
        "\n",
        "The problem that I want to train a machine learning algorithm on is **“To determine what type an animal is of in the zoo based on its given attributes.”**\n",
        "\n",
        "I want to determine this to enable the user to easily classify an animal and if it is of a certain type (Type 5). To train the machine learning algorithm on this problem statement, I will first remove the ‘animal name’ and ‘animal type’ attribute and will solely be using the other 16 attributes for each animal in the zoo. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This problem is* supervised machine learning* as the problem statement asks the algorithm to predict a discrete value by identifying the data inputted as a member of a particular animal type. In the training dataset of animal types, each animal would be pre-labelled as a particular animal type, between 1 and 7, and the algorithm is subsequently evaluated on how accurately it can successfully classify new animals based on their attributes given. Additionally, this is a classification problem as the animals are restricted to the 7 provided animal types given in the dataset, and are already pre-defined and therefore there is no other option or animal type available for them to be defined as. \n",
        "\n",
        "\n",
        "Fundamentally, I am trying to solve this problem as it will save time for the Zoo staff as they won’t have to continuously spend unnecessary time trying to classify each animal in the zoo and any new animals that enter the zoo, but I am primarily doing this to prevent animals from being situated in the incorrect compound as this could lead to incompatible animals being placed together and not getting along. Therefore, once I am able to solve this problem, by predicting the type of each animal, I will hopefully be able to allow the zookeepers to spend more time with the animals and not in the office doing admin, subsequently providing the animals with more in depth care and interaction to allow them to be as happy as they can be and as well looked after as possible. \n"
      ]
    },
    {
      "metadata": {
        "id": "22P-Ft0jGTB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKUXMyka2nVt",
        "colab_type": "code",
        "outputId": "d05fdf7a-a6c4-437e-9397-f7d070dba8d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "csv_file_uri = \"https://raw.githubusercontent.com/lilyadamek8/zoodata/master/Zoo%20Raw%20Data.txt\" ## the location of the data from GitHub\n",
        "\n",
        "## defining the columns needed in the imported dataset \n",
        "column_names = [\n",
        "    \"Animal_Name\", \"Hair\", \"Feathers\", \"Eggs\", \"Milk\", \"Airborne\", \"Aquatic\",\n",
        "    \"Predator\", \"Toothed\", \"Backbone\", \"Breathes\", \"Venomous\", \"Fins\",\n",
        "    \"Legs\", \"Tail\", \"Domestic\", \"Catsize\", \"Animal_Type\"\n",
        "]  \n",
        "\n",
        "data = pd.read_csv(csv_file_uri, names=column_names, index_col=False)\n",
        "\n",
        "data.plot.bar(x='Animal_Type', y = 'Legs')\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFUCAYAAACOQI1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAUAwAACFwAAA/4/YEUZoIx6yQMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f082544ae48>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IExCiF7uA_mR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I used the above code to create a basic bar graph surrounding the animal type against the number of legs an animal type has; I did this before the data was encoded/changed because it was more diverse and I also wanted to ensure that the data was correct and was running as expected. \n"
      ]
    },
    {
      "metadata": {
        "id": "4CG4PNIdAb-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.drop([\"Animal_Name\"], axis=1, inplace=True) # removing the column containing the animal names as it is not required\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YesgTt2L-ZR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Encoding Data**\n",
        "\n",
        "Before I could use a model, I had to encode my dataset by altering the data in the column ‘Animal_Type’.\n",
        "\n",
        "I encoded this data as it originally is a number between 1 and 7 for each animal, however I need to gather information on all of the animals which are of animal type 5, so therefore I altered the data so that if ‘Animal_Type’ = 5 then it was changed to a 1 and every other animal was labelled 0; this enables me to then calculate the accuracy of my machine learning algorithm. I didn’t encode any other column of data as they are all Boolean and do not require encoding as they are already in the necessary format. Before I use any model, I will need to process the raw data and also encode the data as previously discussed, so that it is ready to be used within the model."
      ]
    },
    {
      "metadata": {
        "id": "M7Hht_HFNaUB",
        "colab_type": "code",
        "outputId": "dc29e5f5-7d99-45ed-da99-0cf012ffa48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        }
      },
      "cell_type": "code",
      "source": [
        "def convert_animaltype_integer (num): # converting the animals in type 3 to a boolean value\n",
        "  if num == 3:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0 \n",
        "\n",
        "  \n",
        "data[\"Animal_Type\"] = data.Animal_Type.apply(convert_animaltype_integer) ## changes the value in the column 'animal type' to a 1 for al type 3 animals\n",
        "\n",
        "\n",
        "data.head(30) ## prints out however many rows (animals) that you want to see\n",
        "# change this as appropriate\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hair</th>\n",
              "      <th>Feathers</th>\n",
              "      <th>Eggs</th>\n",
              "      <th>Milk</th>\n",
              "      <th>Airborne</th>\n",
              "      <th>Aquatic</th>\n",
              "      <th>Predator</th>\n",
              "      <th>Toothed</th>\n",
              "      <th>Backbone</th>\n",
              "      <th>Breathes</th>\n",
              "      <th>Venomous</th>\n",
              "      <th>Fins</th>\n",
              "      <th>Legs</th>\n",
              "      <th>Tail</th>\n",
              "      <th>Domestic</th>\n",
              "      <th>Catsize</th>\n",
              "      <th>Animal_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Hair  Feathers  Eggs  Milk  Airborne  Aquatic  Predator  Toothed  \\\n",
              "0      1         0     0     1         0        0         1        1   \n",
              "1      1         0     0     1         0        0         0        1   \n",
              "2      0         0     1     0         0        1         1        1   \n",
              "3      1         0     0     1         0        0         1        1   \n",
              "4      1         0     0     1         0        0         1        1   \n",
              "5      1         0     0     1         0        0         0        1   \n",
              "6      1         0     0     1         0        0         0        1   \n",
              "7      0         0     1     0         0        1         0        1   \n",
              "8      0         0     1     0         0        1         1        1   \n",
              "9      1         0     0     1         0        0         0        1   \n",
              "10     1         0     0     1         0        0         1        1   \n",
              "11     0         1     1     0         1        0         0        0   \n",
              "12     0         0     1     0         0        1         1        1   \n",
              "13     0         0     1     0         0        0         1        0   \n",
              "14     0         0     1     0         0        1         1        0   \n",
              "15     0         0     1     0         0        1         1        0   \n",
              "16     0         1     1     0         1        0         1        0   \n",
              "17     1         0     0     1         0        0         0        1   \n",
              "18     0         0     1     0         0        1         1        1   \n",
              "19     0         0     0     1         0        1         1        1   \n",
              "20     0         1     1     0         1        0         0        0   \n",
              "21     0         1     1     0         1        1         0        0   \n",
              "22     1         0     0     1         0        0         0        1   \n",
              "23     0         1     1     0         1        0         0        0   \n",
              "24     0         0     1     0         0        0         0        0   \n",
              "25     0         0     1     0         0        1         1        1   \n",
              "26     0         0     1     0         0        1         1        1   \n",
              "27     1         0     0     1         1        0         0        1   \n",
              "28     1         0     0     1         0        0         0        1   \n",
              "29     1         0     0     1         0        0         1        1   \n",
              "\n",
              "    Backbone  Breathes  Venomous  Fins  Legs  Tail  Domestic  Catsize  \\\n",
              "0          1         1         0     0     4     0         0        1   \n",
              "1          1         1         0     0     4     1         0        1   \n",
              "2          1         0         0     1     0     1         0        0   \n",
              "3          1         1         0     0     4     0         0        1   \n",
              "4          1         1         0     0     4     1         0        1   \n",
              "5          1         1         0     0     4     1         0        1   \n",
              "6          1         1         0     0     4     1         1        1   \n",
              "7          1         0         0     1     0     1         1        0   \n",
              "8          1         0         0     1     0     1         0        0   \n",
              "9          1         1         0     0     4     0         1        0   \n",
              "10         1         1         0     0     4     1         0        1   \n",
              "11         1         1         0     0     2     1         1        0   \n",
              "12         1         0         0     1     0     1         0        0   \n",
              "13         0         0         0     0     0     0         0        0   \n",
              "14         0         0         0     0     4     0         0        0   \n",
              "15         0         0         0     0     6     0         0        0   \n",
              "16         1         1         0     0     2     1         0        0   \n",
              "17         1         1         0     0     4     1         0        1   \n",
              "18         1         0         0     1     0     1         0        1   \n",
              "19         1         1         0     1     0     1         0        1   \n",
              "20         1         1         0     0     2     1         1        0   \n",
              "21         1         1         0     0     2     1         0        0   \n",
              "22         1         1         0     0     4     1         0        1   \n",
              "23         1         1         0     0     2     1         0        1   \n",
              "24         0         1         0     0     6     0         0        0   \n",
              "25         1         1         0     0     4     0         0        0   \n",
              "26         1         1         1     0     4     0         0        0   \n",
              "27         1         1         0     0     2     1         0        0   \n",
              "28         1         1         0     0     4     1         0        1   \n",
              "29         1         1         0     0     2     0         1        1   \n",
              "\n",
              "    Animal_Type  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             0  \n",
              "4             0  \n",
              "5             0  \n",
              "6             0  \n",
              "7             0  \n",
              "8             0  \n",
              "9             0  \n",
              "10            0  \n",
              "11            0  \n",
              "12            0  \n",
              "13            0  \n",
              "14            0  \n",
              "15            0  \n",
              "16            0  \n",
              "17            0  \n",
              "18            0  \n",
              "19            0  \n",
              "20            0  \n",
              "21            0  \n",
              "22            0  \n",
              "23            0  \n",
              "24            0  \n",
              "25            0  \n",
              "26            0  \n",
              "27            0  \n",
              "28            0  \n",
              "29            0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "9969WVXOmzeI",
        "colab_type": "code",
        "outputId": "3a74e9b2-4cef-4e07-e7e3-b72ba4dbe7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# displaying the size of the dataset\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "KBWhaVH-4ep5",
        "colab_type": "code",
        "outputId": "6c8cdcea-51a6-414d-b8f2-cd822729a656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "feature_columns = data.columns.tolist()\n",
        "feature_columns.remove(\"Animal_Type\")\n",
        "\n",
        "X = data[feature_columns].values # gets the values/data from all of the columns\n",
        "y = data[\"Animal_Type\"].values # gets the animal type values (either 1 or 0)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 16)\n",
            "(101,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pZtK6UKqE8Fv",
        "colab_type": "code",
        "outputId": "6c145504-401d-44c5-faab-bfc1f2b35f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "X, y = digits.data, digits.target\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# classify small against large digits\n",
        "y = (y > 4).astype(np.int)\n",
        "\n",
        "\n",
        "# Set regularization parameter\n",
        "for i, C in enumerate((100, 2, 14)):\n",
        "  \n",
        "    # turn down tolerance for short training time\n",
        "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=500)\n",
        "    clf_l1_LR.fit(X, y)\n",
        " \n",
        "\n",
        "    coef_l1_LR = clf_l1_LR.coef_.ravel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ufNsfWDyFoqh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "N.B. I took this code and manipulated from the following website:\n",
        "\n",
        "http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py"
      ]
    },
    {
      "metadata": {
        "id": "wHmWQ66_e3Wl",
        "colab_type": "code",
        "outputId": "fca8c69f-9da3-4c04-934f-99019b095ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# #applying cross-validation to the dataset\n",
        "from sklearn import model_selection\n",
        "\n",
        "seed = 3\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.008, random_state=seed) ## alter the test size for a change in prediction accuracy\n",
        "\n",
        "# training the model using the training sets\n",
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# using the training data to predict the result\n",
        "accuracy_train = clf.score(X_train, y_train)\n",
        "print(\"- The model's training accuracy is {}\".format(accuracy_train*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- The model's training accuracy is 91.07744107744108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rNGFymdmxo-F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having experimented with altering the 'test size' used, I found that the larger the test size that was used, the more accurate the models training would be. For example, when I use test size of 0.2 the model's training accuracy is only 95, however when I increase the test size used to 0.5, the training of the model becomes 98% accurate from the training."
      ]
    },
    {
      "metadata": {
        "id": "0aRF1e9eJ7bD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Linear Regression Model**"
      ]
    },
    {
      "metadata": {
        "id": "B0v6IDoEyIL5",
        "colab_type": "code",
        "outputId": "eecc261a-3ab6-4a2e-a394-395abad5d0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "# Makes predictions using the testing set\n",
        "pred = clf.predict(X[-1,:].reshape(1,64))\n",
        "\n",
        "print(data.iloc[data.index[0]])\n",
        "if pred == 0:\n",
        "  print(\"\\nPredicts its not a Type 3 animal\") ## to easily display to the user what the system predicts, rather than it just showing a 1 or a 0 value \n",
        "else :\n",
        "  print(\"Predicts it is a Type 3 animal\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hair           1\n",
            "Feathers       0\n",
            "Eggs           0\n",
            "Milk           1\n",
            "Airborne       0\n",
            "Aquatic        0\n",
            "Predator       1\n",
            "Toothed        1\n",
            "Backbone       1\n",
            "Breathes       1\n",
            "Venomous       0\n",
            "Fins           0\n",
            "Legs           4\n",
            "Tail           0\n",
            "Domestic       0\n",
            "Catsize        1\n",
            "Animal_Type    0\n",
            "Name: 0, dtype: int64\n",
            "Predicts it is a Type 3 animal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mqXB-Zmh9RdE",
        "colab_type": "code",
        "outputId": "c425b822-fa7e-4468-94a7-02b12c379a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "# Creating the decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(max_depth=8)\n",
        "\n",
        "clf.fit(X, y)\n",
        "\n",
        "pred2 = clf.predict(X[-1,:].reshape(1,64))\n",
        "print(data.iloc[data.index[0]])\n",
        "if pred == 0:\n",
        "  print(\"\\nPredict its not a Type 3 animal\")\n",
        "else :\n",
        "  print(\"Predict it is a Type 3 animal\")\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hair           1\n",
            "Feathers       0\n",
            "Eggs           0\n",
            "Milk           1\n",
            "Airborne       0\n",
            "Aquatic        0\n",
            "Predator       1\n",
            "Toothed        1\n",
            "Backbone       1\n",
            "Breathes       1\n",
            "Venomous       0\n",
            "Fins           0\n",
            "Legs           4\n",
            "Tail           0\n",
            "Domestic       0\n",
            "Catsize        1\n",
            "Animal_Type    0\n",
            "Name: 0, dtype: int64\n",
            "Predict it is a Type 3 animal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ACckC7OD7w5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This breaks down the thought process of the machine to the user, which allows them to see which columns were most important to the machine when predicting the type that the animal would be.\n",
        "\n",
        "The columns that are chosen are relevant to the animal type, for example animal type 5 contains mainly aquatic animals and therefore the main columns that will be taken into consideration would be aquatic, feathers, legs, tail, etc. as they are the most relevant to the examples of animals already considered to be of that type (5). This enables the machine learning to be the most accurate as possible with its predictions and focus on the key data."
      ]
    },
    {
      "metadata": {
        "id": "bs0o5AKWtQ9-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I chose to do animal type 3 as it has the most feature columns which makes it the most diverse for the users and the most accurate as it takes in more information and data than the other types of animals. "
      ]
    },
    {
      "metadata": {
        "id": "4WnBLtwrrCM0",
        "colab_type": "code",
        "outputId": "f58f4b08-76c4-4b44-bc0a-66b69750482c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "## printing out the importance of the feature columns one by one\n",
        "\n",
        "for ix, c in enumerate(feature_columns):\n",
        "    if clf.feature_importances_[ix] > 0:\n",
        "        print(\"Column {} is {}\".format(c, clf.feature_importances_[ix]*100.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column Milk is 0.10886701070432009\n",
            "Column Aquatic is 0.9195986737638934\n",
            "Column Predator is 11.224771452446381\n",
            "Column Breathes is 1.0265711677019962\n",
            "Column Fins is 0.9004316129235612\n",
            "Column Legs is 1.0453130151884023\n",
            "Column Tail is 0.5206696922502579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OD2U1p2W6jVR",
        "colab_type": "code",
        "outputId": "b0717b4e-5990-455f-ef10-cbb9783540d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "# applying the 2 models/algorithms to the dataset \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "scoring = 'accuracy'\n",
        "\n",
        "models = []\n",
        "models.append(('LogisticReg', LogisticRegression()))\n",
        "models.append(('DecisionTree', DecisionTreeClassifier()))\n",
        "\n",
        "# evaluating each model individually to get the accuracy rates\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "  \n",
        "  kfold = model_selection.KFold(n_splits=10, random_state=0.3) \n",
        "  cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
        "  \n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticReg: 0.879745 (0.042027)\n",
            "DecisionTree: 0.863669 (0.031338)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ep7k8fVQIrJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameter Tuning Results\n",
        "\n",
        "See below for the results from when I altered the hyper-parameters that the algorithm was using (in the form of 'tolerance'), and the data that it resulted in. This includes the data for both the average accuracy rate and also the standard deviation rate and how they differed when the parameters were tuned.\n",
        "\n",
        "Run No.  |  Hyper-Parameter  |  Mean Result  ||\t Standard Deviation\n",
        "                  \n",
        "\n",
        "                                                      \n",
        "1                   0.00001\t                       0.865\t                    0.027\n",
        "\n",
        "2                 \t0.001                           \t0.866\t                    0.031\n",
        "\n",
        "3\t                  0.008\t                            0.873                    \t0.028\n",
        "\n",
        "4\t                  0.009\t                            0.870\t                    0.028\n",
        "\n",
        "5\t                  0.01\t                             0.870\t                    0.034\n",
        "\n",
        "6\t                  0.5\t                                0.862\t                     0.036\n",
        "\n",
        "7\t                  5 (i)\t                              0.869\t                     0.026\n",
        "\n",
        "8\t                  5 (ii)\t                            0.865\t                     0.024\n",
        "\n",
        "9\t                  5 (iii)\t                            0.870\t                     0.033\n",
        "\n",
        "10\t               5(iv)\t                            0.877\t                     0.029\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HDb9JlW6DXUJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Comparing Models\n",
        "\n",
        "I have chosen to use 2 models to assist with my problem statement, which are a ‘Decision Tree’ and ‘Logistic Regression’. To help me to decide which model would be the best and most accurate for my zoo dataset, I used box and whisker plot graphs to visually compare the 2 models. This compared the range and the mean accuracy of both models which made it easy for me to be able to see how accurate they were on average and also generally, as the results will differ when the algorithm is run. \n",
        "\n",
        "There is a population of accuracy measures for each algorithm as each algorithm was evaluated 10 times (10 fold cross validation). "
      ]
    },
    {
      "metadata": {
        "id": "LHFmQeg77iFg",
        "colab_type": "code",
        "outputId": "df9fe506-514c-45cf-9431-2b0b682cb724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "# Comparing the 2 models \n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAGQCAYAAACDPkUVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAYRFwBAAAY9H8Bcxb7p0/8quMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f07fa0937f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Nl7ZNPZSlweB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results from running the models**\n",
        "\n",
        "The above graph displays visual evidence from running the models, based on their performance and accuracy. Take this as a comparison, as you can see the most accurate prediction and least accurate which therefore allows the user to see the range in which the predictions can be in. I personally would choose the model with the lowest range (so the decision tree in this instance) as it is more consistent and has lower variance which could lead to anomalies or even just incorrect results. \n",
        "\n",
        "\n",
        "I ran both of the models 12 times, including cross-validation. I tried different hyper parameters in order to reach maximum accuracy and to improve my machine learning algorithm. Please find the hyper parameters that I used and the corresponding results below for each model.\n"
      ]
    },
    {
      "metadata": {
        "id": "bkc-tXfR6EZw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Clustering Algorithm**\n",
        "**Mean Shift Clustering**\n",
        "\n",
        "I added the ‘Mean Shift Clustering Algorithm’ to my machine learning experiment's pre-processing using the below code, in order to remove duplicates and also to find the most common predictions in the form of clusters by collating the nearest possible answers/results. \n",
        "\n",
        "+ I used this clustering algorithm rather than the other 3 due to its simplicity. Due to the end result being controlled by one parameter (the kernel bandwith value), whereas for example the K-means clustering algorithm requires the number of clusters as an input and I wouldn't have this data to be able to input it initially. \n",
        "+ I also chose to use this algorithm as it uses the density of the points to generate the number of clusters which provides a more rounded and accurate result to the algorithm.\n"
      ]
    },
    {
      "metadata": {
        "id": "MQYtH-TCTm4W",
        "colab_type": "code",
        "outputId": "d302ce66-96b3-4038-bf79-9bb47125c314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import cluster\n",
        "import numpy as np\n",
        "\n",
        "ms = cluster.MeanShift(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=1)\n",
        "ms.fit(data,y=None)\n",
        "labels= ms.labels_\n",
        "\n",
        "cluster_centers = ms.cluster_centers_\n",
        "\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "\n",
        "print(\"Number of estimated clusters : %d\" % n_clusters_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of estimated clusters : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7WKcZRJd9dXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code uses the mean shift clustering algorithm to initially generate the number of the clusters formed by the dataset, allowing the user to be able to visualise the amount of clusters that have been created and the general outline of the clustering diagram created from the models."
      ]
    },
    {
      "metadata": {
        "id": "kSJx-75wB9-J",
        "colab_type": "code",
        "outputId": "2b8cb5cb-2b31-4929-f1d6-5c118dbde87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.random.randn(10)\n",
        "y = np.random.randn(10)\n",
        "Cluster = np.array([0, 1, 1, 1, 3, 2, 2, 3, 0, 2])    # Labels of cluster 0 to 3\n",
        "centers = np.random.randn(4, 2) \n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(x,y,c=Cluster,s=50)\n",
        "for i,j in centers:\n",
        "    ax.scatter(i,j,s=50,c='purple',marker='X') ## defines what colour and marking should be used when showing the main clusters on the diagram\n",
        "\n",
        "# defines the axis titles\n",
        "ax.set_xlabel('Animal Type Predictions')\n",
        "ax.set_ylabel('Clusters') \n",
        "\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
            "  \"matplotlib is currently using a non-GUI backend, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAF3CAYAAACCI8B9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQXABAACGQXABAACG8f8BjbSc08TDhOAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f081fa9bb00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "t2cpL_NDoVq_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see from the above, I successfully created and implemented a mean shift clustering algorithm based on the previous code block which generates the number of clusters that the dataset will generate.\n",
        "\n",
        "However, I didn't manage to incorporate my own zoo dataset into this algorithm and the plot graph above includes random data based on random clusters of data. But despite all of this, the plot graph was successfully implemented and with further knowledge on how to implement a clustering algorithm, this would become personalised to generating predictions on animal type very quickly. I would simply need to feed the clusters into my machine learning algorithm, using the models, and the clustering algorithm would then be incorporated successfully. \n",
        "\n",
        "The plot graph above clearly displays the clusters (using the purple 'X' notation), but it also shows each point (prediction) that has been taken into account for the algorithm to be able to form the initial clusters.  This information is useful as it allows the user to see where the clusters were formed on the graph i.e. allows them to visualise where the most common predictions were made and then were the subsequent clusters were formed and the justifications behind their formations. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qGuUb87dMZjF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Visualisation\n",
        "\n",
        "I ran the models countless times, however I changed the hyper-parameters to attempt to increase the accuracy and I recorded the results each time in correspondence to the tolerance value in the algorithm. You can see on my [GitHub](https://github.com/lilyadamek8/GraphsData/blob/master/graph2.PNG) a visual representation of the data and how the mean accuracy changed when the hyper parameter was altered. This graph contains both the mean accuracy rate of the algorithm's predictions but also the standard deviation of each. As you can see, the standard deviation stayed nearly the same rate all throughout with slight deviations but the average accuracy rate was the main change in predictions data.\n",
        "\n",
        "\n",
        "As you can see from the [graph](https://https://github.com/lilyadamek8/GraphsData/blob/master/mean%20graph.PNG), as the hyper-parameter increased (from 0.00001 to 0.008), the accuracy rate slowly increased therefore showing that the higher the tolerance the faster that the machine would learn and the more accurate that it would be with it's predictions; however, as you can also see, 0.008 appeared to be the most accurate value to use for tolerance as the accuracy on average decreased from there onwards. I then decided to use the same tolerance value and not change any hyper-parameter but instead to re-run the experiment with the same tolerance and see if the results varied and ultimately if my machine was learning anything from the experiment (I hoped so!). Luckily, I was proved to be correct as you can see when I ran the algorithm with 'Tolerance = 5', the average accuracy did increase (I am aware there was a decrease in 5 (ii) but this could be due to an anomaly or even just a bad prediction, I am just stating that thereon after the mean increased with the amount of runs). Therefore, the machine learning experiment is successful as it learns every time it is run and  is always gaining knowledge from the algorithm and dataset!"
      ]
    },
    {
      "metadata": {
        "id": "Yzxj26j0NoPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "I chose to use the Logistic Regression and Decision Tree models as once I had loaded the data into my system, I ran around 5 individual models and gathered the accuracy of each of their predictions, and these 2 models were by far the most accurate which is why I chose to go with them for my experiment. In conclusion to this task, I would choose to use the ‘Decision Tree’ model to allow the machine learning algorithm to predict if the animal would be of a particular type, mainly because it has a lower range and still has a accuracy rate of 95% which shows that the results are more likely to be correct and closer to the actual result as the algorithm is more consistent than the ‘Logistic Regression’ model. \n",
        "\n",
        "Both models generate an accuracy rate of around 95% each time (ranging between 85%-100%, depending on the hyper-parameters used), which is very high but is expected as the animals can only be of types 1-7 and the given animals in each type are all very similar already and have very similar attributes, making it a lot easier to classify them and provide a prediction on what type an animal would be based on its given attributes. The algorithm is also made a lot simpler by the majority of the attributes being made up of Boolean values, meaning that there is only a true or false answer that is possible which therefore makes the predictions a lot simpler and the structure more straight forward for the algorithm to process and follow. Due to these factors, I do not think that it is an issue that the accuracy is so high, because the possible values of the attributes are limited and subsequently lead to the process being made a lot simpler than other datasets that could've been used with different algorithms. \n",
        "\n",
        "https://github.com/lilyadamek8/graph1/blob/master/111.png\n",
        "\n",
        "As you can see from the whisker plot graph above (click to view the file), the ‘Decision Tree’ model is the more consistent of the 2, with fewer varying results and a lower range, making it the most reliable model to use as there is less chance of anomalies occurring and more chance of receiving an accurate prediction. However, the ‘Logistic Regression’ model is also extremely accurate with a relatively low range, however the lower extreme generated from the results is a lot lower than the decision tree model, and therefore the LR model appears to provide less consistent results. Overall, the robustness of the Decision Tree model is observably better than the Logistic Regression model however this could also mean that the machine is learning more from the LR model due to the varying results (it could be taking more data as an input and processing a lot more data which could be causing the varying results) and therefore this model would be the most accurate and representative of the dataset and would be the preferred model to use; this would require some further investigation to be able to actually decipher why the results were so varied in the LR model and what data each model prioritises when making a prediction on the animal type. Despite all of these factors, neither of the models have any anomalies from the data, which shows that both models are reliable and provide accurate and well processed results most of the time (if not all) and therefore both models could be used for making predictions for this experiment. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Recommendation\n",
        "\n",
        "\n",
        "My advice would be that we actively use the Decision Tree model for this experiment as it provides a very reliable prediction nearly every time, the range is small, meaning that the results do not vary by a vast amount proving that the results are nearly ALWAYS 90% or more accurate, and there are no anomalies. I recommend that we use this model to predict if an animal is of a certain type as it runs extremely fast, saving zookeepers time that could be spent doing other work and will save the zoo animals from being placed in the incorrect pens with incompatible animal types, but most importantly because it has a very high accuracy rate and is reliable, consistent and robust which is everything that a machine learning algorithm should be.\n"
      ]
    }
  ]
}